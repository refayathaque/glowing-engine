Paragraph 1

- All apps, be they web or mobile, have a “backend” they communicate with to enable users to do things like authenticate, fetch data, add data, etc.
- The layer in between the app, a.k.a. client, and the backend is an API, short for [Application Programming Interface](https://www.freecodecamp.org/news/what-is-an-api-in-english-please-b880a3214a82/).
- Google Cloud is a cloud provider just like Microsoft Azure and AWS, you can use those services to create APIs as well, but this guide focuses purely on the Google Cloud implementation.
- Google Cloud defines an [API](https://cloud.google.com/api-gateway/docs/about-api-gateway#what_is_an_api) as, "...an interface that makes it easy for one application to consume capabilities or data from another application. By defining stable, simple, and well-documented entry points, APIs enable developers to easily access and reuse application logic built by other developers."
- In most software development teams there are frontend developers building the client, and coding the client to make [RESTful](https://restfulapi.net/) API calls via HTTP to the backend
- While frontend developers build the client, backend developers are building out the API for the client to be able to access the backend.
- Frontend developers only need to know the API endpoints their client needs to hit to perform required functions, these endpoints are published as part of the API by the backend engineers.

Paragraph 2

- This API will allow users to create buckets, list buckets, list objects, rename object, delete buckets in Google Cloud Storage.
- E.g., developers can use the endpoints we publish by creating this API to have their apps, or clients, interact with Google Cloud Storage.
- Google storage is a managed service that allows you to store any kind of files in groupings called "buckets", this is essentially the same as AWS' S3, or Simple Storage Service.

Paragraph 3

- While there are many approaches to API development, we’ve adopted the Google Cloud prescribed methodology, which dictates the use of their API Gateway service and the [Swagger OpenAPI Specification Version 2](https://swagger.io/docs/specification/basic-structure/).
- This Specification is essentially a YAML file describing the API, as you’ll see later, this file will be used by the API Gateway Google Cloud resource to deploy our API and act as the aforementioned layer between the client and backend.
- There are several benefits to using API Gateway, as is explained in the Google Cloud documentation. "API Gateway enables you to provide secure access to your services through a well-defined REST API that is consistent across all of your services, regardless of service implementation. A consistent API: Makes it easy for app developers to consume your services and enables you to change the backend service implementation without affecting the public API."
- Due to the importance of APIs we decided to develop this prototype as scaffolding for all future applications. The flexibility of this methodology means that we can use the same scaffolding to build other APIs acting as intermediaries between other client types like IoT and all other backend services, not just web servers running on virtual machines. Having an API implementation of this nature makes it backend agnostic. For example, in this exercise we will create a Node.js web server running as a Docker container in Google Cloud Run responding to client requests, but if we change our mind and decide to use Serverless Cloud Functions as the backend instead of Cloud Run we can, and all we’ll need to do is change the “backend” argument in the spec YAML file.
- The code samples can be found [here](https://github.com/refayathaque/gcp-infra-and-microservices)
- APIs allow for full decoupling of concerns between frontend and backend engineers, and the GCP methodology takes it even further with decoupling between the API and the backend layers, the API spec author just needs the endpoint for whatever service backend engineers will chose to use. The standardization and uniformity achieved with APIs and the OpenAPI Specificiation results in improved software delivery velocity.

Paragraph 4

- Create a Google Cloud account, and in the process you will be guided to create a project, keep the project id handy as we will need it in subsquent configuration setup.
- Before getting started you'll need to set up your local environment to deploy code and provision infrastructure in Google Cloud, follow the instructions on these links to get this done: Installing the [Cloud SDK](https://cloud.google.com/sdk/docs/install), [initializing](https://cloud.google.com/sdk/docs/initializing), and [authorizing](https://cloud.google.com/sdk/docs/authorizing).
- "Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files." "Write infrastructure as code using declarative configuration files. HashiCorp Configuration Language (HCL) allows for concise descriptions of resources using blocks, arguments, and expressions." "Apply changes to hundreds of cloud providers with terraform apply to reach the desired state of the configuration."
  - Why use it?
    - "Codify your application infrastructure - Reduce human error and increase automation by provisioning infrastructure as code."
    - "Create reproducible infrastructure Provision consistent testing, staging, and production environments with the same configuration."
- ^ all from terraform homepage
- Use this [link](https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/getting_started) to set up Terraform in your local environment and ready to provision infrastructure in your project.
  - The sample code's `prototype-b` directory contains all the Terraform code you will need to build this API, for this initial set up portion of this exercise, copy over the `provider.tf` file to your local working directory and run the Terraform commands from this working directory. You'll see that the provider references Terraform variables and that's something you can set up using the code [here](https://github.com/refayathaque/somtum.io-infra-and-images#steps), all you need to do is fill in the default values.
    - The first three values you can find in the Google Cloud console
    - The service account key will be the path name to the `.json` file, you can create this key and download it using this [link](https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating), set the role to Editor (under "Basic")
      - We will discuss IAM and Service Accounts a little more in depth in later parts of this guide, for now just know that this is something you need to authorize Terraform to create infrastructure in your Google Cloud project.
      - I advise you to keep it in the local working directory (same as where your `provider.tf` is)

Paragraph 5 (Container Image)

- A Docker "[container](https://www.docker.com/resources/what-container) is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings." Using Docker ensures that the code we write in our local machines will work in the environment Cloud Run (which I'll describe below) is running.
- The code which we will dockerize is using [Express.js](https://expressjs.com/) and the Google Cloud Storage SDK to run a Node.js web application that responds to HTTP methods and returns data, writes data, etc. Express.js is excellent for creating API backends because it has "a myriad of HTTP utility methods and middleware at your disposal, creating a robust API is quick and easy." The main Express.js code can be found in the `server.js` file within the container image [code](https://github.com/refayathaque/somtum.io-infra-and-images/tree/main/nodejs-containers/storage-crud)
- The Node.js web server we’ll be deploying is performing some very basic Google Cloud Storage tasks like providing the client abilities to create buckets, delete buckets, delete objects, list buckets, list objects, and change object names. The entry file is `server.js` and it’s leveraging Express.js to create routes for the client to hit. There are modules corresponding to each route to do things like fetch buckets, delete buckets, etc. When the time comes to provision our entire API infrastructure, we will first create this docker image and deploy it to Google Cloud for Cloud Run to later use in provisioning itself. For now it’s important to understand what this image is doing, and that’s returning Google Cloud Storage data to the client by leveraging the JavaScript Google Cloud Storage client. However, it’s important to discuss, albeit in short, the security implications of this web server. As you’ll see in the infrastructure provisioning section, this Cloud Run service running the Node.js Express web server will have limited permissions, permissions only to do the storage operations it needs to do, nothing more.
- Our intention with this code is for others to take and tweak as necessary to maybe have a Node.js app that interacts with other Google Cloud services or even other Cloud provider's services. If you want to for example, create an app that can interact with Machine Learning services you just have to add the Node.js library for that service and write the code.
- Cloud Run is akin to AWS' Elastic Container Service if you're more familiar with that, it's essentially a service that allows you to "develop and deploy highly scalable containerized applications on a fully managed serverless platform", and its also highly cost-effective due to it's "serverless" nature, you "only pay when your code is running." We will use this service to run our Docker container image that will interact with Google Cloud Storage.

Paragraph 6 (Intro to infra provisioning)

- In deploying this API we will be taking a two-step approach to infrastructure deployment, and that is due to the way Cloud Run services work. Since Cloud Run needs to know what container image to use we need to first build and push the image to Google Cloud Artifact Registry. Once the image is deployed we can proceed and build out all the other infrastructure pieces listed below.

Paragraph 7 (Infra part 1)

- `apis.tf`
  - We need to enable a bunch of Google Cloud services we will need to provision the infrastructure, without enabling these services Terraform will not be able to provision and you'll get errors. The Terraform code is written in a way where we've sequence the provisioning of all resources to only occur if their respective APIs have been enabled.
- "[Artifact Registry](https://cloud.google.com/artifact-registry) is a single place for your organization to manage container images and language packages (such as Maven and npm). It is fully integrated with Google Cloud’s tooling and runtimes and comes with support for native artifact protocols. This makes it simple to integrate it with your CI/CD tooling to set up automated pipelines."
- What does the `run-me-first.sh` script do?
  - You have to change the container image build variables before running this script, your region and project id will differ, if you clone/fork this the entire repo then you can keep the image name and artifact registry repo name as they are
  - Enables Artifact Registry API, and you can find this Terraform resource in the `apis.tf` file
  - Runs ONLY the part of our Terraform code that provisions the Artifact Registry repository, this resource is in the `artifact_registry.tf`. It's important to note here that Terraform themselves frown upon this practice of using `terraform apply -target` to provision **select** resources, but until we can find a better approach for Docker container image provisioning to Cloud Run we will maintain this approach.
  - Builds our Docker Node.js container image and pushes it to Artifact Registry repository created in the previous step.
- At this point, the bash script above executed all commands successfully, you will have provisioned the Artifact Registry repository and pushed the Node.js container image into it. Therefore we are now ready to provision all remaining infrastructure, especially the Cloud Run service which needed the container image to be available from Artifact Registry.
- Enable app that links Google Cloud to GitHub for Continuous Delivery with Cloud Build to work, Terraform will not be able to provision the Cloud Build resource without you having done this first.

Paragraph 8 (Infra part 2) Provisioning Google Cloud infrastructure with Terraform

- We won't go into too much detail talking about the Terraform code, as it's all discernible upon reading the [documentation](https://registry.terraform.io/providers/hashicorp/google/latest/docs), however, it would be remiss of us if we didn't mention that it's not the best, it's pretty apparent that Hashicorp (the creators of Terraform) have decided to allocate more of their engineering prowess on AWS and Azure. In this section of the guide, we will briefly touch upon each of the `.tf` files you see starting with `apis.tf` and ending with `iam.tf`.
- The `apis.tf` file is provisioning all the service APIs we will need to provision all our resources.
- The `cloud_run.tf` file is provisioning the Cloud Run service, and the key thing to note here is the `containers` block where we declare what image it needs to use when provisioning itself.
- The `cloud_build.tf` file is provisioning our Continuous Delivery pipeline, essentially, we are using Cloud Build to deploy the latest version of the docker container image every time we update the container image source code in GitHub. But we cannot over emphasize how important it is for you to enable the Github app in the Google Cloud console first, and this was mentioned above in part 1 of our infrastructure provisioning. Cloud Build relies on a YAML descriptor delineates the build steps, and looking at the `cloudbuild.yaml` you'll see that we're building the container image with Docker and then deploying it to Cloud Run using `gcloud`. Please also pay close attention to the `filename` and `included_files` arguments, based on how you structure your project, these paths could be different.
- The `api_gateway.tf` file is provisioning the API, the gateway and the config. While the the API itself serves as the parent component of the architecture, the gateway and the config sit within it as child components.
  - "Using [API Gateway](https://cloud.google.com/api-gateway/docs/about-api-gateway#api-gateway), app developers consume your REST APIs to implement apps. Because all APIs are hosted on API Gateway, app developers see a consistent interface across all backend services. By deploying your APIs on API Gateway, you can update the backend service, or even move the service from one architecture to another, without having to change the API. As long as the API to your service stays consistent, app developers will not have to modify deployed apps because of underlying changes on your backend."
  - "An API defined on API Gateway consists of [two main components](https://cloud.google.com/api-gateway/docs/deployment-model): API config: The API configuration created when you upload an API definition. You create the API definition as an OpenAPI spec. If your API manages gRPC services on Cloud Run, you can define your API with a gRPC service definition and configuration. Each time you upload an API definition, API Gateway creates a new API config. That is, you can create an API config **but you cannot later modify it**. If you later edit the API definition in the OpenAPI spec or gRPC service definition, and then upload the edited API definition, you create a new API config. Gateway: An Envoy-based, high-performance, scalable proxy that hosts the deployed API config. Deploying an API config to a gateway creates the external facing URL that your API clients use to access the API."
  - More details on the OpenAPI Spec YAML file this resource relies will be talked about in the following section.
  - If everything here has been set up correctly, Terraform, at the end of provisioning all our infrastructure will output a URL, and this will be the base URL that we used to test out the API and all its functionalities later on.
- The `iam.tf` file is provisioning Google Cloud Identity Access Management (IAM) resources, these resources basically provide the infrastructure pieces we're provisioning with the authorization required to interact with each other and allow us access to them. IAM is very nuanced and those working as Cloud Engineers have to understand it _very_ thoroughly as security is always paramount, but for the purposes of this exercise, the rudimentary explanation above will suffice, if you'd like to know more you can check out the IAM documentation [here](https://cloud.google.com/iam/docs/overview). The three main parts of IAM are Principal, Role and Policy, and for you to make sense of what you see in the code here are definitions for each.
  - "A principal can be a Google Account (for end users), a service account (for applications and compute workloads), a Google group, or a Google Workspace account or Cloud Identity domain that can access a resource. The identity of a principal is an email address associated with a user, service account, or Google group; or a domain name associated with a Google Workspace account or a Cloud Identity domain."
  - "A role is a collection of permissions. Permissions determine what operations are allowed on a resource. When you grant a role to a principal, you grant all the permissions that the role contains."
  - "The IAM policy is a collection of role bindings that bind one or more principals to individual roles. When you want to define who (principal) has what type of access (role) on a resource, you create a policy and attach it to the resource."
- The `storage.tf` file is provisioning 3 buckets for the purposes of testing at the end, and only one of these buckets will have an object inside, with the object being a `test.txt` file.

Paragraph 9 (OpenAPI Spec YAML)

- "An OpenAPI document describes the surface of your REST API, and defines information such as: The name and description of the API, The individual endpoints (paths) in the API, How the callers are authenticated"
- "API Gateway supports APIs that are described using the OpenAPI specification, version 2.0. Your API can be implemented using any publicly available REST framework such as Django or Jersey. You describe your API in a YAML file referred to as an OpenAPI document. This page describes some of the benefits to using OpenAPI, shows a basic OpenAPI document, and provides additional information to help you get started with OpenAPI. One of the primary benefits to using OpenAPI is for documentation; once you have an OpenAPI document that describes your API, it is easy to generate reference documentation for your API. There other benefits to using OpenAPI. For example, you can: Generate client libraries in dozens of languages, Generate server stubs. Use projects to verify your conformance and to generate samples."
- Our YAML file can be found in the directory `api-configs` within the `prototype-b` directory in the repo, and upon opening it you'll see that we wrote this YAML based on guidance from the Google Cloud [API Gateway](https://cloud.google.com/api-gateway/docs/openapi-overview) and [OpenAPI](https://swagger.io/docs/specification/2-0/basic-structure/) documentation. The YAML file also matches how we've set up the Express.js routes in the container image source [code](https://github.com/refayathaque/somtum.io-infra-and-images/blob/main/nodejs-containers/storage-crud/server.js). For example, the GET method to fetch all objects in a bucket is described by the path `/objects/{bucket}` with the parameter being `{bucket`, and it's the same in our container image codebase's `server.js` file, `app.get("/objects/:bucket", async (req, res) => {`.

Paragraph 10 (Testing)

- Now that we've gone over the code and the main concepts, we can actually provision the second group of infrastructure, so run `terraform apply --auto-approve` and wait and see your Google Cloud services get created! Terraform could throw some errors in relation to API Gateway, and if you just try to provision again using the command above it should be successful the second time around.
- To test that our API is working as expected we’ll take the API Gateway default hostname returned by Terraform and append the routes described in the open api spec file, inserting parameters wherever necessary.

Last Paragraph (Conclusion)

- For help with local testing please reach out
- If something doesn't work also reach out and I'll try to help you out as much as possible
- Ideal scenario we want to block direct access to the Cloud Run service via the endpoint, and only allow the public internet access to it via the API Gateway endpoint, this is something we might explore and provide as an update to this guide.

GO AND MANUALLY DELETE ALL RESOURCES EXCEPT API ENABLEMENT
LOST TF STATE FILES AND TF LIBS, NEED TO REINIT AND TESTtest

DLP Code Dump is where Dataflow compiled code is
Michelle needs a calendar
Update timesheets because Radha signed off yesterday
